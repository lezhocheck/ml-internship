{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 09:20:24.081833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/lezhocheck/opt/anaconda3/envs/py3tf2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMNIST dataset is used for training the model that recognizes handwritten VIN-code characters.\n",
    "##### It is one of the best options to choose for training, because it is enough big and very suitable for this kind of problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='emnist',\n",
      "    full_name='emnist/byclass/3.0.0',\n",
      "    description=\"\"\"\n",
      "    The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset.\n",
      "    \n",
      "    Note: Like the original EMNIST data, images provided here are inverted horizontally and rotated 90 anti-clockwise. You can use `tf.transpose` within `ds.map` to convert the images to a human-friendlier format.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    EMNIST ByClass\n",
      "    \"\"\",\n",
      "    homepage='https://www.nist.gov/itl/products-and-services/emnist-dataset',\n",
      "    data_path='/Users/lezhocheck/tensorflow_datasets/emnist/byclass/3.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=535.73 MiB,\n",
      "    dataset_size=349.16 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=62),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=116323, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=697932, num_shards=4>,\n",
      "    },\n",
      "    citation=\"\"\"@article{cohen_afshar_tapson_schaik_2017,\n",
      "        title={EMNIST: Extending MNIST to handwritten letters},\n",
      "        DOI={10.1109/ijcnn.2017.7966217},\n",
      "        journal={2017 International Joint Conference on Neural Networks (IJCNN)},\n",
      "        author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Schaik, Andre Van},\n",
      "        year={2017}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 09:20:27.043546: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "emnist, info = tfds.load('emnist', with_info=True, as_supervised=True)\n",
    "emnist_train, emnist_test = emnist['train'], emnist['test']\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 62\n"
     ]
    }
   ],
   "source": [
    "NUM_VALIDATION = int(0.1 * len(emnist_train))\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 1000\n",
    "OUTPUT_SIZE = info.features['label'].num_classes\n",
    "NUM_EPOCHS = 12\n",
    "\n",
    "print(f'Number of classes: {OUTPUT_SIZE}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datset contains handwritten digits, uppercase letters and lowercase letters. VIN contains only uppercase letters and digits, but some letters can be written as lowercase, so it is better to classify them as lowercase letters and then convert them to uppercase if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale grayscale image to bound it pixel values between 0..1\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image / 255., label\n",
    "\n",
    "scaled_train_validation_data = emnist_train.map(scale)\n",
    "scaled_test_data = emnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data and creating a validation dataset to prevent overfitting during training\n",
    "shuffled_scaled_train_validation_data = scaled_train_validation_data.shuffle(BUFFER_SIZE)\n",
    "shuffled_test_data = scaled_test_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_scaled_train_validation_data.take(NUM_VALIDATION)\n",
    "train_data = shuffled_scaled_train_validation_data.skip(NUM_VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching samples\n",
    "batched_train_data = train_data.batch(BATCH_SIZE)\n",
    "batched_validation_data = validation_data.batch(NUM_VALIDATION)\n",
    "batched_test_data = shuffled_test_data.batch(len(shuffled_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1700x1100 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert targets to actual character values\n",
    "def to_chr(label):\n",
    "    if label < 10:\n",
    "        return str(label)\n",
    "    elif label < 36:\n",
    "        return chr(label + 55)\n",
    "    return chr(label + 61)\n",
    "\n",
    "# Let's display some of the dataset entries\n",
    "def show_one(img, label, ax):\n",
    "    img = np.fliplr(img)\n",
    "    img = np.rot90(img, k=1)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.text(0.05, 1, f'label: {to_chr(label)}', fontsize=14, color='red')\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "display_count = 5\n",
    "fig, axs = plt.subplots(1, display_count, figsize=(17, 11))\n",
    "for ind, value in enumerate(train_data.take(display_count)):\n",
    "    img, label = value[0].numpy(), value[1].numpy()\n",
    "    show_one(img, label, axs[ind])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(batched_validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with adam optimizer and \n",
    "# sparse_categorical_crossentropy loss function \n",
    "# (we have categorical targets that are not one-hot encoded)\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "629/629 - 22s - loss: 0.8467 - accuracy: 0.7515 - val_loss: 0.5798 - val_accuracy: 0.8091 - 22s/epoch - 36ms/step\n",
      "Epoch 2/12\n",
      "629/629 - 22s - loss: 0.5177 - accuracy: 0.8255 - val_loss: 0.5007 - val_accuracy: 0.8304 - 22s/epoch - 35ms/step\n",
      "Epoch 3/12\n",
      "629/629 - 22s - loss: 0.4667 - accuracy: 0.8381 - val_loss: 0.4655 - val_accuracy: 0.8409 - 22s/epoch - 35ms/step\n",
      "Epoch 4/12\n",
      "629/629 - 22s - loss: 0.4393 - accuracy: 0.8448 - val_loss: 0.4545 - val_accuracy: 0.8427 - 22s/epoch - 35ms/step\n",
      "Epoch 5/12\n",
      "629/629 - 22s - loss: 0.4201 - accuracy: 0.8498 - val_loss: 0.4446 - val_accuracy: 0.8444 - 22s/epoch - 35ms/step\n",
      "Epoch 6/12\n",
      "629/629 - 22s - loss: 0.4058 - accuracy: 0.8534 - val_loss: 0.4396 - val_accuracy: 0.8466 - 22s/epoch - 35ms/step\n",
      "Epoch 7/12\n",
      "629/629 - 22s - loss: 0.3942 - accuracy: 0.8563 - val_loss: 0.4309 - val_accuracy: 0.8499 - 22s/epoch - 35ms/step\n",
      "Epoch 8/12\n",
      "629/629 - 22s - loss: 0.3837 - accuracy: 0.8595 - val_loss: 0.4268 - val_accuracy: 0.8504 - 22s/epoch - 35ms/step\n",
      "Epoch 9/12\n",
      "629/629 - 22s - loss: 0.3756 - accuracy: 0.8613 - val_loss: 0.4265 - val_accuracy: 0.8509 - 22s/epoch - 35ms/step\n",
      "Epoch 10/12\n",
      "629/629 - 22s - loss: 0.3678 - accuracy: 0.8637 - val_loss: 0.4272 - val_accuracy: 0.8512 - 22s/epoch - 35ms/step\n",
      "Epoch 11/12\n",
      "629/629 - 22s - loss: 0.3613 - accuracy: 0.8650 - val_loss: 0.4253 - val_accuracy: 0.8529 - 22s/epoch - 35ms/step\n",
      "Epoch 12/12\n",
      "629/629 - 22s - loss: 0.3542 - accuracy: 0.8673 - val_loss: 0.4264 - val_accuracy: 0.8511 - 22s/epoch - 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc15054d660>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding early stopping to prevent possible overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "model.fit(batched_train_data, epochs=NUM_EPOCHS, verbose=2, \n",
    "          batch_size=BATCH_SIZE,\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training finished with the accuracy 0.86 and validation accuracy 0.85. Let's check model performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 0.4284 - accuracy: 0.8501\n",
      "Test loss: 0.4284268021583557\n",
      "Test accuracy: 0.8500898480415344\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(batched_test_data)\n",
    "print(f'Test loss: {loss}')\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test accuracy is around 0.85, which is probably good result. \n",
    "* However, further improvements can be made by hyperparameter tuning or using more sophisticated models (i.e., CNNs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gen(img):\n",
    "    result = np.fliplr(img)\n",
    "    result = np.rot90(result, k=1)\n",
    "    return tf.cast(result * 255, tf.uint8)\n",
    "\n",
    "sample = scaled_test_data.shuffle(BUFFER_SIZE)\n",
    "# jpeg entries\n",
    "for value in sample.take(5):\n",
    "    img, label = value[0].numpy(), value[1].numpy()\n",
    "    img = prepare_gen(img)\n",
    "    encoded = tf.image.encode_jpeg(img, quality=100)\n",
    "    tf.io.write_file(f'./test_data/character_{to_chr(label)}.jpg', encoded)\n",
    "\n",
    "# png entries\n",
    "for value in sample.take(5):\n",
    "    img, label = value[0].numpy(), value[1].numpy()\n",
    "    img = prepare_gen(img)\n",
    "    encoded = tf.image.encode_png(img)\n",
    "    tf.io.write_file(f'./test_data/character_{to_chr(label)}.png', encoded)\n",
    "\n",
    "# scaled entries\n",
    "for value in sample.take(5):\n",
    "    img, label = value[0].numpy(), value[1].numpy()\n",
    "    img = prepare_gen(img)\n",
    "    new_size = tf.cast(np.random.uniform(0.5, 3) * 28, tf.int32)\n",
    "    img = tf.image.resize(img, [new_size, new_size])\n",
    "    img = tf.cast(img, tf.uint8)\n",
    "    encoded = tf.image.encode_png(img)\n",
    "    tf.io.write_file(f'./test_data/character_{to_chr(label)}.jpeg', encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
